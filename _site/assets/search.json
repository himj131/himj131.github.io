

[
  
  
    
    
      {
        "title": "Kafka 기본 개념",
        "excerpt": "카프카란\n카프카는 Pub/Sub 모델 기반의 메시징 서버로, 링크드인에서 자체적으로 사용하기 위해 만든 분산 데이터 스트림 플랫폼이다.\n기존 메세징 시스템에 비해 처리량이 높고, 파티셔닝이나 복제기능을 갖고 있어 대규모 메시지 처리에 적합한 시스템이다.\n\n일반적인 서비스 회사에서 서비스를 운영할 때 다음과 같은 데이터 시스템들이 필요하다.\n\n  사용량, 응답시간, 에러카운트등의 메트릭 모니터링용 데이터 시스템\n  배치/분석을 위한 로그 저장용 데이터 시스템\n  서비스에 필요한 메인 데이터 시스템\n  key/value 저장소\n\n\n그리고 그 규모가 커질수록 다음 모습처럼 더 복잡한 구조를 띄게 된다.\n\n\n\n카프카는 사내 모든 데이터시스템이나 마이크로서비스와 연결된 이벤트/데이터 스트림 파이프라인의 중앙허브 역할을 목적으로 개발되었고,\n카프카 로고에도 이런 철학이 잘 반영되어 있다.\n\n\n\nKafka 특징\n프로듀서와 컨슈머의 분리(pub/sub)\npub/sub방식이 아닌 경우를 우선 생각해보자. \n서비스를 제공하는 회사에서 웹서버, 앱서버용, 데이터베이스 서버등의 각 모니터링 시스템을 필요해 구축한다.\n일대일로 통신하고 있던 모니터링 서버에 문제가 생겨 웅답이 늦어지는 경우가 발생하면, \n연쇄 작용으로 모니터링 서버와 연결된 다른 서비스 서버들에서도 지연 이슈가 발생하게 된다.\n카프카를 중앙 시스템으로 놓고 데이터 스트림을 필요한 모든 시스템과 애플리케이션으로 데이터를 주고받는 구조는 더 단순할 뿐만아니라\n각 서비스들은 모니터링이나 분석 시스템의 상태와 관계 없이 카프카로 메시지를 보내는 역할만 하면되고,\n마찬가지로 모니털이이나 분석 시스템들도 서비스 서버들의  상태에 상관 없이 카프카에 저장된 메시지만 가져오면 된다.\n\n멀티 프로듀서, 멀티 컨슈머\n프로듀서는 하나 이상의 토픽에 메세지를 보낼수 있고, 컨슈머 역시 하나 이상의 토픽으로부터 메세지를 읽어올 수 있다.\n하나의 토픽에 여러 프로듀서나 컨슈머가 접근이 가능하므로 하나의 데이터를 다양한 용도로 사용하기에 좋다.\n\n  토픽: 데이터가 저장되는 일종의 카테고리\n\n\n메세지 영속성\n일반적인 메시징 시스템에서는 컨슈머가 메시지를 읽어가면 큐에서 바로 메시지를 삭제한다. \n하지만 카프카는 일정 주기만큼 메시지를 디스크에 저장해둔다.\n따라서 컨슈머의 메시지 처리가 늦어지거나 컨슈머의 중단이 발생하더라도 메시지의 손실이 없으며, \n한번 읽은 메시지를 여러번 다시 읽을 수 있다.\n\n확장성(Scalability)\n카프카 클러스터는 3개(권장)의 브로커에서 수십대의 브로커로 확장이 가능하다. 확장 작업도 서비스 중단없이 비교적 간단하게 진행할 수 있다.\n\n빠른 처리 속도\nOS의 페이지 캐시를 사용해 데이터를 읽고 쓰기 때문에 속도가 빠르다.\n또한, 카프카는 작은 메시지들을 묶어서 배치로 처리하기 때문에 I/O로 인한 속도 저하를 개선 시켰다.\n\n고가용성(High availability), 결함에 강함(Fault-tolerant)\n카프카는 분산 시스템을 사용하기 때문에 단일 시스템보다 더 높은 성능을 얻을 수 있고 시스템 확장이 용이하다. 또한 분산시스템 중 하나의 서버에 \n장애가 발생해도 다른 서버가 데이터 손실 없이 작업을 대신한다. 심지어 한대의 서버만 남아도 서비스가 가능하다.\n\nKafka 구성 요소와 역할\n\n\n\n  Zookeeper(주키퍼)\n    \n      카프카는 분산 애플리케이션을 관리하는 코디네이션으로 주키퍼를 사용한다.\n      주키퍼는 분산 각 애플리케이션의 정보를 관리하고, 동기화등을 처리한다.\n      분산 애플리케이션들은 각각 클라이언트가 되어 주키퍼 서버들과 커넥션을 맺어 상태정보 등을 주고 받는다.\n      상태 정보들은 주키퍼 지노드(znode)라 불리는곳에 key-value 형태로 저장되고 이값을 가지고 분산 애플리케이션들은 서로 데이터를 주고 받는다.\n    \n  \n  Broker(브로커)\n    \n      카프카 애플리케이션이 설치되어 있는 서버로 데이터 저장소 역할을 한다.\n    \n  \n  Topic(토픽) 과 Partition(파티션)\n    \n      토픽은 데이터를 구분하는 논리적 개념이고, 파티션은 토픽을 분할한 데이터 저장소 이다.\n      카프카의 리플리케이션은 토픽이 아닌 파티션을 리플리케이션 하는 것이다.\n      파티션 수는 한 번 설정하면 증가만 가능하고 줄일 수는 없다. 토픽을 삭제 해야 한다.\n    \n  \n  Producer(프로듀서)\n    \n      메시지를 생산하는 애플리케이션\n      각 메시지를 토픽 파티션에 매핑하고 파티션의 리더에 요청을 보낸다.\n    \n  \n  Consumer(컨슈머)\n    \n      저장된 메시지를 가져가서 사용하는 서버 또는 애플리케이션\n      특정 파티션을 관리하고 있는 파티션 리더에게 메시지 가져오기 요청을 한다.\n      각 요청에 오프셋을 명시할 수 있어, 컨슈머는 가져올 메시지의 위치를 조정할 수 있고, 이미 가져온 데이터도 다시 가져올 수 있다.\n    \n  \n\n\nKafka 데이터 모델\n토픽\n토픽은 메시지가 저장되는 카테고리와 같다.\n카프카 클러스터는 토픽이란 곳에 데이터를 저장하고,\n프로듀서들은 원하는 토픽에 메시지를 보내고, 컨슈머들도 원하는 토픽에 접근하여 메시지를 가져온다.\n\n\n파티션\n파티션은 토픽을 분할한 대상이다. 카프카의 복제 기능은 이 파티션 단위로 이루어지며, 카프카의 병렬 처리를 가능하게 해 \n효율적인 메시지 전송을 하도록 한다.\n\n\n위 그림상에서 각 메시지 전송속도가 1초일때 파티션이 하나 뿐이라면 3개의 메시지를 전달하는데 총 3초가 걸리게 된다.\n토픽을 파티션을 3개로 나누면 3초를 1초로 줄일 수 있다.\n\n파티션 수가 많아지면 그만큼 빠른 메시지 전송이 가능하긴 하지만 그에따른 단점도 있다.\n\n파티션 수가 많아질 때 단점\n\n  파일 핸들러 낭비\n각 파티션은 브로커의 디렉토리와 매핑이 되고, 저장되는 데이터마다 2개의 파일이 있다. \n카프카는 모든 디렉토리에 대해 파일 핸들을 열게 되어 파티션 수가 많을 수록 파일 핸들 수 역시 많아져\n리소스를 낭비하게 된다.\n  장애 복구 시간 증가\n카프카는 파티션 단위로 데이터를 복제하는데, 파티션들중 하나는 리더이고 나머지는 팔로워이다.\n카프카의 브로커가 다운되면 해당 브로커에 있는 파티션들은 일시적으로 사용 불가 상태가 된다.\n이때 해당 브로커에 리더 파티션이 있다면 다른 브로커로 리더를 이동시켜 요청을 계속 처리하는데, 파티션이 많을수록 그만큼 리더선출 시간이 오래 걸리게 된다.\n컨트롤러로 지정된 브로커가 이 리더의 이동작업을 수행하게 되는데,\n최악의 경우 컨트롤러 역할을 하는 브로커가 다운된 경우에는 살아있는 브로커로 컨트롤러의 페일오버가 작동하고, \n새 컨트롤러가 초기화 하는 동안 주키퍼에서 모든 파티션의 데이터를 읽어야 한다.\n그만큼 장애 시간이 늘어날 수 있다.\n\n\n적절한 파티션\n적절한 파티션 수는 서비스에서 처리할 데이터량을 기준으로 잡는것이 좋다.\n주의할 점은 프로듀서, 파티션, 컨슈머가 처리하는 메시지 양이 비슷해야 메시지 처리가 원하는만큼 효율적으로 처리 된다.\n또한 파티션수의 증가는 아무때나 할 수 있지만, 줄이는 방법은 토픽삭제 외에는 없으므로 한번에 많은 파티션 수를 설정하기 보다는\n운영을 하다가 병목현상이 발생할때 하나씩 늘려가는 방법이 좋다.\n\n카프카 데이터 저장 방식\n파티션에 메시지가 저장되는 위치를 오프셋(offset)이라고 하는데\n오프셋은 순차적으로 증가하는 숫자이며 파티션 내에서 유일한 값이다.\n\n\n\n하나의 토픽을 3개의 파티션으로 나눈 모습으로, 파티션0, 파티션1, 파티션2 두모두 0번 오프셋을 가지고 있다.\n카프카는 이 오프셋을 가지고 메시지의 순서를 보장한다. 컨슈머가 데이터를 가져갈 때는 오프셋 순서대로만 가져갈 수 있다.\n\n리플리케이션 동작방식\n리플리케이션 팩터\n카프카의 리플리케이션 팩터(Replication Factor)를 설정하여 파티션의 복제수를 결정할 수 있다.\n  기본값은 1이며, 토픽별로 서로다른 리플리케이션 팩터값을 설정할 수 있고, 운영 중에도 토픽의 리플리케이션 팩터 값은 변경 할 수 있다.\n  클러스터 내 모든 브로커에 동일하게 설정해야 한다.\n  구성된 리플리케이션들중 원본을 가진 파티션을 리더, 복제한 파티션을 팔로워라 부른다.  \n  모든 읽기와 쓰기는 리더에서만 이루어지고 팔로워는 리더의 데이터를 복제만 한다. \n  만약에 리더 파티션이 있는 브로커가 다운된다면 팔로워들중 하나가 새로운 리더가 되어 요청에 응답한다.\n리플리케이션 수 결정\n리플리케이션을 사용하게 되면 필요한 저장소 크키도 배가 된다.\n  예를 들어 100G의 메시지를 저장하는 파티션의 복제수를 3으로 했다면 총 300G의 저장소가 필요하게 된다.\n  또한 리플리케이션 수가 늘어나면 리플리케이션에 대한 상태 확인을 하는 백그라운드 작업이 차지하는 리소스 사용량이 증가하게 된다. \n  따라서 데이터의 중요도에 따라 팩터를 2 또는 3으로 결정하는 것이 좋다.\n\n리더와 팔로워 관리\n카프카는 ISR(In Sync Replica)를 통해 파티션의 리더와 팔로워의 데이터 불일치 현상을 방지하고 있다. ISR은 파티션 리더와 리더후보 파티션의 그룹을 말한다.\n리더는 팔로워들이 주기적으로 데이터를 확인을 하고 있는지 체크해 일정 주기만큼 확인 요청이 오지 않는다면, 해당 팔로워의 이상을 감지하고 ISR 그룹에서 해당 팔로워를 추방시킨다.\n\n\n\n1) 한개의 리더와 두개의 팔로워로 ISR을 이루고 있음\n2) 팔로워1은 리더의 데이터를 확인해 저장하지만, 팔로워2는 리더에게 데이터 확인 요청을 보내지 않음\n3) (일정주기(replica.lag.time.max.ms)만큼) 확인요청이 들어오지 않아, 리더는 이를 감지하고 해당 팔로워는 더이상 리더의 역할을 대신할 수 없다고 판단해 ISR 그룹에서 추방함\n\n최악의 시나리오\n“카프카 모든 브로커가 다운되었을 때”\n다음과 같이 세개의 브로커가 모두 다운되었을때의 최악의 시나리오를 가정해보자\n\n\n1) ISR 그룹에 리더와 팔로워1, 팔로워2가 있고 A라는 메시지 복제가 잘 이루어졌다.\n2) 팔로워2가 다운되어 데이터 확인을 못하게 되자 ISR 그룹에서 제외되고 B메시지는 리더와 팔로워 1에만 저장되게 된다.\n3) 후에 팔로워1도 다운되어 데이터 확인요청을 못해 ISR 그룹에서 제외되고 C 메시지는 리더에만 저장된다.\n4) 그 후 리더마저 다운되 메시지를 받을 수 없는 상황이 된다.\n\n현재 모든 브로커가 다운된 상황이며 각 브로커마다 가지고 있는 데이터 값도 모두 다르다.\n이때 선택할 수 있는 옵션은 두가지가 있다.\n1) 마지막 리더가 살아나기를 기다린다.\n    - unclean.leader.election.enable=false\n2) ISR에서 추방되었지만 먼저 살아나면 자동으로 리더가 된다. \n    - unclean.leader.election.enable=true\n\n데이터의 중요도에 따라 두가지 방법중 하나를 선택하면 되는데,\n1)번 옵션을 선택할때  마지막 리더는 모든 브로커가 다운되기 전 리더로 모든 메시지를 가지고 있다.\n따라서 이 리더가 살아날때까지 기다린다면 데이터 유실을 막을 수는 있지만, 해당 리더가 다운되는 시간이 길어질 경우, 장애상황 역시 실어진다는 단점이 있다. \n2)번 옵션은 선택하면 팔로워2가 가장먼서 살아나 리더가 되어 다음 메시지를 이어받게 되면 B메시지와 C메시지의 누락이 발생한다. \n따라서 이 옵션은 메시지 유실이 일부 발생 되어도 서비스를 빠르게 정상화 시키고 싶을때 사용 할 수 있다.\n\n프로듀서(Producer)\n프로듀서는 각각의 메시지를 토픽 파티션에 매핑하고 파티션의 리더에 요청을 보내는 역할을 한다.\n키 값을 정해 해당 키를 가진 모든 메시지를 특정 파티션으로 전송 할 수도 있고,\n키 값을 입력하지 않으면 파티션은 라운드 로빈 방식으로 파티션에 균등하게 분배 된다. \n일반적으로 서버로 메시지를 보내고 성공적으로 도착했는지는 확인하지는 않지만, get()메소드를 통해 \n브로커에 전송한 메시지의 성공실패 여부를 확인 할 수 있다.\n\n  메시지가 성공적으로 전송되지 않으면 예외가 발생하고, 에러가 없다면 RecordMetadata를 얻을 수 있다.\nRecordMetadata 를 이용해 메시지가 저장된 파티션과 오프셋을 알 수 있다.\n\n\n모든 메시지에 대해 응답을 기다리면 시간이 너무 많이 소요되므로 비동기 처리를 통해 메시지 전송에 실패할 경우 \n에러 기록등의 예외 처리를 하는 것도 좋다.\n\n메시지 전송 방법\n프로듀서 옵션중 acks 옵션 설정에 따라 메시지 손실 여부나 전송 속도 등이 달라지게 된다.\n\n  메시지 손실 가능성이 높지만 빠른 전송을 원할 때 (acks = 0)\n프로듀서가 메시지를 전송한 후 카프카의 응답을 확인하지 않고, 다음 메시지를 보낼 준비가 되는 즉시 다음 요청을 보낸다.\n프로듀서만 준비되면 바로 보내므로 매우 빠르게 메시지 전송을 할수 있다\n  메시지 손실 가능성이 적으면서 적당한 전송을 원할 때 (acks = 1)\n프로듀서가 카프카로 메시지를 보낸 후 보낸 메시지를 카프카가 잘 받았는지 확인을 한다.\n응답을 기다리는 시간만큼 속도는 저하된다.\n브로커 프로듀서로부터 메시지를 받고 프로듀서에게 acks를 보낸후 바로 장애가 발생하는 경우, 팔로워들이 데이터를 복제하기 전이라면\n새로운 리더가 선출되는 과정에서 데이터 유실이 있을 수 있다.\n  전송 속도는 느리지만 메시지 손실이 없어야 할 때 (acks = all)\n프로듀서가 메시지를 전송하고 난 후 리더가 메시지를 받았는지 확인을 하고 추가로 팔로워까지 메시지를 받았는지 확인하는 방법이다.\n속도는 가장 느리지만 팔로워들의 메시지 저장까지 확인하므로 데이터 손실이 없다. \n이 기능을 완벽히 사용하려면 브로커의 설정도 같이 조정해줘야 한다.\n    \n      프로듀서의 acks=all 과 브로커의 min.insync.replicas   \nmin.insync.replicas 는 메시지를 받고 acks를 보내기 전 확인하는 리플리케이션 수이다. \n이 값에 따라 역시 동작 속도가 달라지는데 1(디폴트 값)로 설정되어 있을 경우 하나의 리플리케이션만 확인하므로\n리더가 메시지를 저장하자마자 acks를 보내게 된다.\nmin.insync.replicas 값을 브로커 수와 동일하게 설정하게 되면, 브로커가 한대만 다운되도 카프카 클러스터 전체 장애와 비슷한 상황이 발생하게 된다.\n    \n  \n\n\n컨슈머(Consumer)\n프로듀서가 카프카 토픽으로 메시지를 보내면 그 토픽의 메시지를 가져와서 소비하는 역할을 하는 애플리케이션이다.\n컨슈머 주요 기능은 특정 파티션을 관리하고 있는 파티션 리더에게 메시지 가져오기 요청을 하는 것이다.\n각 요청은 오프셋을 명시하여 그 위치로부터 메시지를 수신하고, 특정 파티션을 지정해 데이터를 가져올 수도 있다. \n또한 가져왔던 메시지들을 다시 가져 올 수도 있고, 한꺼번에 여러 토픽으로부터 데이터를 읽어올 수도 있다.\n\n파티션과 메시지 순서\n카프카에서 토픽의 파티션이 여러개인 경우, 메시지의 순서를 보장할 수 없다. \n아래 예를 보자\n\n\n\n프로듀서에서 “abcde12345” 순서대로 메시지로 보냈다면, 카프카는 기본적으로 라운드로빈 방식으로 파티션에 데이터를 저장하므로\n위 그림과 같은 모습으로 데이터가 저장된다. 그리고 컨슈머를 통해 데이터를 읽어온다면 \n“ad14be25c3”순으로 읽어오게 된다.\n컨슈머에서의 메시지 순서는 동일한 파티션 내에서는 프로듀서가 생성한 순서와 동일하게 처리되지만 파티션과 파티션 사이에서는 순서가 보장되지 않는다.\n메세지의 순서를 완전히 보장 받고 싶다면 토픽의 파티션 수를 1로 설정해 사용하면 되지만 분산 처리를 할 수 없고, 하나의 컨슈머에서만 처리할 수 있기 때문에 \n처리량이 높지 않다.\n\n컨슈머 그룹\n하나의 토픽에는 여러 컨슈머 그룹이 동시에 접속해 메시지를 가져올 수 있다. \n컨슈머 그룹 안에서 컨슈머들은 메시지를 가져오는 토픽의 파티션에 대해 소유권을 공유 한다.\n다음 그림은 파티션 수가 3인 토픽에서 컨슈머가 하나인 컨슈머 그룹이 메시지를 가져오는 모습이다.\n\n프로듀서가 쌓는 메시지 양이 많아져 컨슈머가 읽어가지 못하는 메시지들이 쌓이면, 다음과 같이 컨슈머를 추가해 확장해야 한다.\n\n추가 컨슈머인 컨슈머02, 컨슈머03을 시작할때 컨슈머 그룹 아이디만 컨슈머1과 동일하게 설정하면 위 그림과 같은 구성이 되고,\n파티션1의 소유권은 컨슈머02로, 파티션2의 소유권은 컨슈머 03으로 이동했다. 이렇게 소유권을 이동하는 것을 리밸런스(rebalance)라고 한다.\n리밸런스를 하는 동안 일시적으로 컨슈머는 메시지를 읽어 올 수 없다.\n그리고 토픽의 파티션에는 동일 컨슈머 그룹 내에서 하나의 컨슈머만 연결 할 수 있기 때문에 파티션 수를 초과하는 컨슈머를 추가 할 때에는 \n반드시 파티션 수도 함께 늘려줘야 한다.\n\n컨슈머가 일정한 주기로 하트비트를 보내으로써 이 소유권을 유지한다.\n하트비트는 컨슈머가 poll 할 때와 가져간 메시지의 오프셋을 커밋할 때 보낸다.\n오랫동안 컨슈머가 하트비트를 보내지 않으면 세션은 타임아웃되고 해당 컨슈머가 다운되었다고 판단하여 리밸런스를 진행하고 해당 컨슈머를 컨슈머 그룹에서 제외 시킨다. \n이렇게 되면 제외된 컨슈머가 소유하고 있던 파티션은 다른 컨슈머에게 넘어가게 되고, \n다음 그림처럼 하나의 컨슈머가 두개의 파티션으로부터 메시지를 가져오는 불균형한 상황이 발생할 수 도 있다.\n\n여러 컨슈머 그룹들이 하나의 토픽에서 메시지를 가져갈 수 있는데 그 이유는 컨슈머 그룹마다 각자의 오프셋을 별도로 관리하기 때문이다. \n컨슈머 그룹아이디만 중복되지 않게 조심하면 된다.\n\n커밋과 오프셋\n컨슈머 그룹의 컨슈머들은 각각의 파티션에 자신이 가져간 메시지의 오프셋을 기록하고 있다. \n각 파티션에 대해 현재 위치를 업데이트 하는 동작을 ‘커밋한다’고 표현한다.\n컨슈머 그룹내에 컨슈머가 추가되거나 삭제될 때 그룹 내에서 리밸런스가 일어나고, 리밸런스가 일어난 후 각각의 컨슈머는 이전에 처리한 파티션에 상관없이 \n새로운 파티션을 할당 받는다. \n새로운 파티션에서 가장 최근에 커밋된 오프셋을 읽고 그 이후 데이터를 가져오는데,\n만약 커밋된 오프셋이 실제 처리한 오프셋보다 작으면 커밋된 오프셋부터 실제 처리한 오프셋까지의 메시지는 중복으로 처리가 된다.\n반대로 커밋된 오프셋이 실제 처리한 오프셋보다 크면 커밋된 오프셋부터 실제 처리한 오프셋까지의 메시지는 누락된다. \n커밋방법에는 자동커밋과 수동커밋 두가지 방법이 있다.\n1) 자동커밋\n5초마다 컨슈머는 poll()을 호출할 때 가장 마지막 오프셋을 커밋한다. 커밋되기전 리밸런스가 일어나면 중복 데이터가 발생 할 수 있다.\n2) 수동커밋\n커밋을 원하는 시점에 commitSync() 를 호출해 커밋한다. 역시나 중복 데이터가 발생할 수 있지만 손실은 없다.\n\n",
        "content": "카프카란\n카프카는 Pub/Sub 모델 기반의 메시징 서버로, 링크드인에서 자체적으로 사용하기 위해 만든 분산 데이터 스트림 플랫폼이다.\n기존 메세징 시스템에 비해 처리량이 높고, 파티셔닝이나 복제기능을 갖고 있어 대규모 메시지 처리에 적합한 시스템이다.\n\n일반적인 서비스 회사에서 서비스를 운영할 때 다음과 같은 데이터 시스템들이 필요하다.\n\n  사용량, 응답시간, 에러카운트등의 메트릭 모니터링용 데이터 시스템\n  배치/분석을 위한 로그 저장용 데이터 시스템\n  서비스에 필요한 메인 데이터 시스템\n  key/value 저장소\n\n\n그리고 그 규모가 커질수록 다음 모습처럼 더 복잡한 구조를 띄게 된다.\n\n\n\n카프카는 사내 모든 데이터시스템이나 마이크로서비스와 연결된 이벤트/데이터 스트림 파이프라인의 중앙허브 역할을 목적으로 개발되었고,\n카프카 로고에도 이런 철학이 잘 반영되어 있다.\n\n\n\nKafka 특징\n프로듀서와 컨슈머의 분리(pub/sub)\npub/sub방식이 아닌 경우를 우선 생각해보자. \n서비스를 제공하는 회사에서 웹서버, 앱서버용, 데이터베이스 서버등의 각 모니터링 시스템을 필요해 구축한다.\n일대일로 통신하고 있던 모니터링 서버에 문제가 생겨 웅답이 늦어지는 경우가 발생하면, \n연쇄 작용으로 모니터링 서버와 연결된 다른 서비스 서버들에서도 지연 이슈가 발생하게 된다.\n카프카를 중앙 시스템으로 놓고 데이터 스트림을 필요한 모든 시스템과 애플리케이션으로 데이터를 주고받는 구조는 더 단순할 뿐만아니라\n각 서비스들은 모니터링이나 분석 시스템의 상태와 관계 없이 카프카로 메시지를 보내는 역할만 하면되고,\n마찬가지로 모니털이이나 분석 시스템들도 서비스 서버들의  상태에 상관 없이 카프카에 저장된 메시지만 가져오면 된다.\n\n멀티 프로듀서, 멀티 컨슈머\n프로듀서는 하나 이상의 토픽에 메세지를 보낼수 있고, 컨슈머 역시 하나 이상의 토픽으로부터 메세지를 읽어올 수 있다.\n하나의 토픽에 여러 프로듀서나 컨슈머가 접근이 가능하므로 하나의 데이터를 다양한 용도로 사용하기에 좋다.\n\n  토픽: 데이터가 저장되는 일종의 카테고리\n\n\n메세지 영속성\n일반적인 메시징 시스템에서는 컨슈머가 메시지를 읽어가면 큐에서 바로 메시지를 삭제한다. \n하지만 카프카는 일정 주기만큼 메시지를 디스크에 저장해둔다.\n따라서 컨슈머의 메시지 처리가 늦어지거나 컨슈머의 중단이 발생하더라도 메시지의 손실이 없으며, \n한번 읽은 메시지를 여러번 다시 읽을 수 있다.\n\n확장성(Scalability)\n카프카 클러스터는 3개(권장)의 브로커에서 수십대의 브로커로 확장이 가능하다. 확장 작업도 서비스 중단없이 비교적 간단하게 진행할 수 있다.\n\n빠른 처리 속도\nOS의 페이지 캐시를 사용해 데이터를 읽고 쓰기 때문에 속도가 빠르다.\n또한, 카프카는 작은 메시지들을 묶어서 배치로 처리하기 때문에 I/O로 인한 속도 저하를 개선 시켰다.\n\n고가용성(High availability), 결함에 강함(Fault-tolerant)\n카프카는 분산 시스템을 사용하기 때문에 단일 시스템보다 더 높은 성능을 얻을 수 있고 시스템 확장이 용이하다. 또한 분산시스템 중 하나의 서버에 \n장애가 발생해도 다른 서버가 데이터 손실 없이 작업을 대신한다. 심지어 한대의 서버만 남아도 서비스가 가능하다.\n\nKafka 구성 요소와 역할\n\n\n\n  Zookeeper(주키퍼)\n    \n      카프카는 분산 애플리케이션을 관리하는 코디네이션으로 주키퍼를 사용한다.\n      주키퍼는 분산 각 애플리케이션의 정보를 관리하고, 동기화등을 처리한다.\n      분산 애플리케이션들은 각각 클라이언트가 되어 주키퍼 서버들과 커넥션을 맺어 상태정보 등을 주고 받는다.\n      상태 정보들은 주키퍼 지노드(znode)라 불리는곳에 key-value 형태로 저장되고 이값을 가지고 분산 애플리케이션들은 서로 데이터를 주고 받는다.\n    \n  \n  Broker(브로커)\n    \n      카프카 애플리케이션이 설치되어 있는 서버로 데이터 저장소 역할을 한다.\n    \n  \n  Topic(토픽) 과 Partition(파티션)\n    \n      토픽은 데이터를 구분하는 논리적 개념이고, 파티션은 토픽을 분할한 데이터 저장소 이다.\n      카프카의 리플리케이션은 토픽이 아닌 파티션을 리플리케이션 하는 것이다.\n      파티션 수는 한 번 설정하면 증가만 가능하고 줄일 수는 없다. 토픽을 삭제 해야 한다.\n    \n  \n  Producer(프로듀서)\n    \n      메시지를 생산하는 애플리케이션\n      각 메시지를 토픽 파티션에 매핑하고 파티션의 리더에 요청을 보낸다.\n    \n  \n  Consumer(컨슈머)\n    \n      저장된 메시지를 가져가서 사용하는 서버 또는 애플리케이션\n      특정 파티션을 관리하고 있는 파티션 리더에게 메시지 가져오기 요청을 한다.\n      각 요청에 오프셋을 명시할 수 있어, 컨슈머는 가져올 메시지의 위치를 조정할 수 있고, 이미 가져온 데이터도 다시 가져올 수 있다.\n    \n  \n\n\nKafka 데이터 모델\n토픽\n토픽은 메시지가 저장되는 카테고리와 같다.\n카프카 클러스터는 토픽이란 곳에 데이터를 저장하고,\n프로듀서들은 원하는 토픽에 메시지를 보내고, 컨슈머들도 원하는 토픽에 접근하여 메시지를 가져온다.\n\n\n파티션\n파티션은 토픽을 분할한 대상이다. 카프카의 복제 기능은 이 파티션 단위로 이루어지며, 카프카의 병렬 처리를 가능하게 해 \n효율적인 메시지 전송을 하도록 한다.\n\n\n위 그림상에서 각 메시지 전송속도가 1초일때 파티션이 하나 뿐이라면 3개의 메시지를 전달하는데 총 3초가 걸리게 된다.\n토픽을 파티션을 3개로 나누면 3초를 1초로 줄일 수 있다.\n\n파티션 수가 많아지면 그만큼 빠른 메시지 전송이 가능하긴 하지만 그에따른 단점도 있다.\n\n파티션 수가 많아질 때 단점\n\n  파일 핸들러 낭비\n각 파티션은 브로커의 디렉토리와 매핑이 되고, 저장되는 데이터마다 2개의 파일이 있다. \n카프카는 모든 디렉토리에 대해 파일 핸들을 열게 되어 파티션 수가 많을 수록 파일 핸들 수 역시 많아져\n리소스를 낭비하게 된다.\n  장애 복구 시간 증가\n카프카는 파티션 단위로 데이터를 복제하는데, 파티션들중 하나는 리더이고 나머지는 팔로워이다.\n카프카의 브로커가 다운되면 해당 브로커에 있는 파티션들은 일시적으로 사용 불가 상태가 된다.\n이때 해당 브로커에 리더 파티션이 있다면 다른 브로커로 리더를 이동시켜 요청을 계속 처리하는데, 파티션이 많을수록 그만큼 리더선출 시간이 오래 걸리게 된다.\n컨트롤러로 지정된 브로커가 이 리더의 이동작업을 수행하게 되는데,\n최악의 경우 컨트롤러 역할을 하는 브로커가 다운된 경우에는 살아있는 브로커로 컨트롤러의 페일오버가 작동하고, \n새 컨트롤러가 초기화 하는 동안 주키퍼에서 모든 파티션의 데이터를 읽어야 한다.\n그만큼 장애 시간이 늘어날 수 있다.\n\n\n적절한 파티션\n적절한 파티션 수는 서비스에서 처리할 데이터량을 기준으로 잡는것이 좋다.\n주의할 점은 프로듀서, 파티션, 컨슈머가 처리하는 메시지 양이 비슷해야 메시지 처리가 원하는만큼 효율적으로 처리 된다.\n또한 파티션수의 증가는 아무때나 할 수 있지만, 줄이는 방법은 토픽삭제 외에는 없으므로 한번에 많은 파티션 수를 설정하기 보다는\n운영을 하다가 병목현상이 발생할때 하나씩 늘려가는 방법이 좋다.\n\n카프카 데이터 저장 방식\n파티션에 메시지가 저장되는 위치를 오프셋(offset)이라고 하는데\n오프셋은 순차적으로 증가하는 숫자이며 파티션 내에서 유일한 값이다.\n\n\n\n하나의 토픽을 3개의 파티션으로 나눈 모습으로, 파티션0, 파티션1, 파티션2 두모두 0번 오프셋을 가지고 있다.\n카프카는 이 오프셋을 가지고 메시지의 순서를 보장한다. 컨슈머가 데이터를 가져갈 때는 오프셋 순서대로만 가져갈 수 있다.\n\n리플리케이션 동작방식\n리플리케이션 팩터\n카프카의 리플리케이션 팩터(Replication Factor)를 설정하여 파티션의 복제수를 결정할 수 있다.\n  기본값은 1이며, 토픽별로 서로다른 리플리케이션 팩터값을 설정할 수 있고, 운영 중에도 토픽의 리플리케이션 팩터 값은 변경 할 수 있다.\n  클러스터 내 모든 브로커에 동일하게 설정해야 한다.\n  구성된 리플리케이션들중 원본을 가진 파티션을 리더, 복제한 파티션을 팔로워라 부른다.  \n  모든 읽기와 쓰기는 리더에서만 이루어지고 팔로워는 리더의 데이터를 복제만 한다. \n  만약에 리더 파티션이 있는 브로커가 다운된다면 팔로워들중 하나가 새로운 리더가 되어 요청에 응답한다.\n리플리케이션 수 결정\n리플리케이션을 사용하게 되면 필요한 저장소 크키도 배가 된다.\n  예를 들어 100G의 메시지를 저장하는 파티션의 복제수를 3으로 했다면 총 300G의 저장소가 필요하게 된다.\n  또한 리플리케이션 수가 늘어나면 리플리케이션에 대한 상태 확인을 하는 백그라운드 작업이 차지하는 리소스 사용량이 증가하게 된다. \n  따라서 데이터의 중요도에 따라 팩터를 2 또는 3으로 결정하는 것이 좋다.\n\n리더와 팔로워 관리\n카프카는 ISR(In Sync Replica)를 통해 파티션의 리더와 팔로워의 데이터 불일치 현상을 방지하고 있다. ISR은 파티션 리더와 리더후보 파티션의 그룹을 말한다.\n리더는 팔로워들이 주기적으로 데이터를 확인을 하고 있는지 체크해 일정 주기만큼 확인 요청이 오지 않는다면, 해당 팔로워의 이상을 감지하고 ISR 그룹에서 해당 팔로워를 추방시킨다.\n\n\n\n1) 한개의 리더와 두개의 팔로워로 ISR을 이루고 있음\n2) 팔로워1은 리더의 데이터를 확인해 저장하지만, 팔로워2는 리더에게 데이터 확인 요청을 보내지 않음\n3) (일정주기(replica.lag.time.max.ms)만큼) 확인요청이 들어오지 않아, 리더는 이를 감지하고 해당 팔로워는 더이상 리더의 역할을 대신할 수 없다고 판단해 ISR 그룹에서 추방함\n\n최악의 시나리오\n“카프카 모든 브로커가 다운되었을 때”\n다음과 같이 세개의 브로커가 모두 다운되었을때의 최악의 시나리오를 가정해보자\n\n\n1) ISR 그룹에 리더와 팔로워1, 팔로워2가 있고 A라는 메시지 복제가 잘 이루어졌다.\n2) 팔로워2가 다운되어 데이터 확인을 못하게 되자 ISR 그룹에서 제외되고 B메시지는 리더와 팔로워 1에만 저장되게 된다.\n3) 후에 팔로워1도 다운되어 데이터 확인요청을 못해 ISR 그룹에서 제외되고 C 메시지는 리더에만 저장된다.\n4) 그 후 리더마저 다운되 메시지를 받을 수 없는 상황이 된다.\n\n현재 모든 브로커가 다운된 상황이며 각 브로커마다 가지고 있는 데이터 값도 모두 다르다.\n이때 선택할 수 있는 옵션은 두가지가 있다.\n1) 마지막 리더가 살아나기를 기다린다.\n    - unclean.leader.election.enable=false\n2) ISR에서 추방되었지만 먼저 살아나면 자동으로 리더가 된다. \n    - unclean.leader.election.enable=true\n\n데이터의 중요도에 따라 두가지 방법중 하나를 선택하면 되는데,\n1)번 옵션을 선택할때  마지막 리더는 모든 브로커가 다운되기 전 리더로 모든 메시지를 가지고 있다.\n따라서 이 리더가 살아날때까지 기다린다면 데이터 유실을 막을 수는 있지만, 해당 리더가 다운되는 시간이 길어질 경우, 장애상황 역시 실어진다는 단점이 있다. \n2)번 옵션은 선택하면 팔로워2가 가장먼서 살아나 리더가 되어 다음 메시지를 이어받게 되면 B메시지와 C메시지의 누락이 발생한다. \n따라서 이 옵션은 메시지 유실이 일부 발생 되어도 서비스를 빠르게 정상화 시키고 싶을때 사용 할 수 있다.\n\n프로듀서(Producer)\n프로듀서는 각각의 메시지를 토픽 파티션에 매핑하고 파티션의 리더에 요청을 보내는 역할을 한다.\n키 값을 정해 해당 키를 가진 모든 메시지를 특정 파티션으로 전송 할 수도 있고,\n키 값을 입력하지 않으면 파티션은 라운드 로빈 방식으로 파티션에 균등하게 분배 된다. \n일반적으로 서버로 메시지를 보내고 성공적으로 도착했는지는 확인하지는 않지만, get()메소드를 통해 \n브로커에 전송한 메시지의 성공실패 여부를 확인 할 수 있다.\n\n  메시지가 성공적으로 전송되지 않으면 예외가 발생하고, 에러가 없다면 RecordMetadata를 얻을 수 있다.\nRecordMetadata 를 이용해 메시지가 저장된 파티션과 오프셋을 알 수 있다.\n\n\n모든 메시지에 대해 응답을 기다리면 시간이 너무 많이 소요되므로 비동기 처리를 통해 메시지 전송에 실패할 경우 \n에러 기록등의 예외 처리를 하는 것도 좋다.\n\n메시지 전송 방법\n프로듀서 옵션중 acks 옵션 설정에 따라 메시지 손실 여부나 전송 속도 등이 달라지게 된다.\n\n  메시지 손실 가능성이 높지만 빠른 전송을 원할 때 (acks = 0)\n프로듀서가 메시지를 전송한 후 카프카의 응답을 확인하지 않고, 다음 메시지를 보낼 준비가 되는 즉시 다음 요청을 보낸다.\n프로듀서만 준비되면 바로 보내므로 매우 빠르게 메시지 전송을 할수 있다\n  메시지 손실 가능성이 적으면서 적당한 전송을 원할 때 (acks = 1)\n프로듀서가 카프카로 메시지를 보낸 후 보낸 메시지를 카프카가 잘 받았는지 확인을 한다.\n응답을 기다리는 시간만큼 속도는 저하된다.\n브로커 프로듀서로부터 메시지를 받고 프로듀서에게 acks를 보낸후 바로 장애가 발생하는 경우, 팔로워들이 데이터를 복제하기 전이라면\n새로운 리더가 선출되는 과정에서 데이터 유실이 있을 수 있다.\n  전송 속도는 느리지만 메시지 손실이 없어야 할 때 (acks = all)\n프로듀서가 메시지를 전송하고 난 후 리더가 메시지를 받았는지 확인을 하고 추가로 팔로워까지 메시지를 받았는지 확인하는 방법이다.\n속도는 가장 느리지만 팔로워들의 메시지 저장까지 확인하므로 데이터 손실이 없다. \n이 기능을 완벽히 사용하려면 브로커의 설정도 같이 조정해줘야 한다.\n    \n      프로듀서의 acks=all 과 브로커의 min.insync.replicas   \nmin.insync.replicas 는 메시지를 받고 acks를 보내기 전 확인하는 리플리케이션 수이다. \n이 값에 따라 역시 동작 속도가 달라지는데 1(디폴트 값)로 설정되어 있을 경우 하나의 리플리케이션만 확인하므로\n리더가 메시지를 저장하자마자 acks를 보내게 된다.\nmin.insync.replicas 값을 브로커 수와 동일하게 설정하게 되면, 브로커가 한대만 다운되도 카프카 클러스터 전체 장애와 비슷한 상황이 발생하게 된다.\n    \n  \n\n\n컨슈머(Consumer)\n프로듀서가 카프카 토픽으로 메시지를 보내면 그 토픽의 메시지를 가져와서 소비하는 역할을 하는 애플리케이션이다.\n컨슈머 주요 기능은 특정 파티션을 관리하고 있는 파티션 리더에게 메시지 가져오기 요청을 하는 것이다.\n각 요청은 오프셋을 명시하여 그 위치로부터 메시지를 수신하고, 특정 파티션을 지정해 데이터를 가져올 수도 있다. \n또한 가져왔던 메시지들을 다시 가져 올 수도 있고, 한꺼번에 여러 토픽으로부터 데이터를 읽어올 수도 있다.\n\n파티션과 메시지 순서\n카프카에서 토픽의 파티션이 여러개인 경우, 메시지의 순서를 보장할 수 없다. \n아래 예를 보자\n\n\n\n프로듀서에서 “abcde12345” 순서대로 메시지로 보냈다면, 카프카는 기본적으로 라운드로빈 방식으로 파티션에 데이터를 저장하므로\n위 그림과 같은 모습으로 데이터가 저장된다. 그리고 컨슈머를 통해 데이터를 읽어온다면 \n“ad14be25c3”순으로 읽어오게 된다.\n컨슈머에서의 메시지 순서는 동일한 파티션 내에서는 프로듀서가 생성한 순서와 동일하게 처리되지만 파티션과 파티션 사이에서는 순서가 보장되지 않는다.\n메세지의 순서를 완전히 보장 받고 싶다면 토픽의 파티션 수를 1로 설정해 사용하면 되지만 분산 처리를 할 수 없고, 하나의 컨슈머에서만 처리할 수 있기 때문에 \n처리량이 높지 않다.\n\n컨슈머 그룹\n하나의 토픽에는 여러 컨슈머 그룹이 동시에 접속해 메시지를 가져올 수 있다. \n컨슈머 그룹 안에서 컨슈머들은 메시지를 가져오는 토픽의 파티션에 대해 소유권을 공유 한다.\n다음 그림은 파티션 수가 3인 토픽에서 컨슈머가 하나인 컨슈머 그룹이 메시지를 가져오는 모습이다.\n\n프로듀서가 쌓는 메시지 양이 많아져 컨슈머가 읽어가지 못하는 메시지들이 쌓이면, 다음과 같이 컨슈머를 추가해 확장해야 한다.\n\n추가 컨슈머인 컨슈머02, 컨슈머03을 시작할때 컨슈머 그룹 아이디만 컨슈머1과 동일하게 설정하면 위 그림과 같은 구성이 되고,\n파티션1의 소유권은 컨슈머02로, 파티션2의 소유권은 컨슈머 03으로 이동했다. 이렇게 소유권을 이동하는 것을 리밸런스(rebalance)라고 한다.\n리밸런스를 하는 동안 일시적으로 컨슈머는 메시지를 읽어 올 수 없다.\n그리고 토픽의 파티션에는 동일 컨슈머 그룹 내에서 하나의 컨슈머만 연결 할 수 있기 때문에 파티션 수를 초과하는 컨슈머를 추가 할 때에는 \n반드시 파티션 수도 함께 늘려줘야 한다.\n\n컨슈머가 일정한 주기로 하트비트를 보내으로써 이 소유권을 유지한다.\n하트비트는 컨슈머가 poll 할 때와 가져간 메시지의 오프셋을 커밋할 때 보낸다.\n오랫동안 컨슈머가 하트비트를 보내지 않으면 세션은 타임아웃되고 해당 컨슈머가 다운되었다고 판단하여 리밸런스를 진행하고 해당 컨슈머를 컨슈머 그룹에서 제외 시킨다. \n이렇게 되면 제외된 컨슈머가 소유하고 있던 파티션은 다른 컨슈머에게 넘어가게 되고, \n다음 그림처럼 하나의 컨슈머가 두개의 파티션으로부터 메시지를 가져오는 불균형한 상황이 발생할 수 도 있다.\n\n여러 컨슈머 그룹들이 하나의 토픽에서 메시지를 가져갈 수 있는데 그 이유는 컨슈머 그룹마다 각자의 오프셋을 별도로 관리하기 때문이다. \n컨슈머 그룹아이디만 중복되지 않게 조심하면 된다.\n\n커밋과 오프셋\n컨슈머 그룹의 컨슈머들은 각각의 파티션에 자신이 가져간 메시지의 오프셋을 기록하고 있다. \n각 파티션에 대해 현재 위치를 업데이트 하는 동작을 ‘커밋한다’고 표현한다.\n컨슈머 그룹내에 컨슈머가 추가되거나 삭제될 때 그룹 내에서 리밸런스가 일어나고, 리밸런스가 일어난 후 각각의 컨슈머는 이전에 처리한 파티션에 상관없이 \n새로운 파티션을 할당 받는다. \n새로운 파티션에서 가장 최근에 커밋된 오프셋을 읽고 그 이후 데이터를 가져오는데,\n만약 커밋된 오프셋이 실제 처리한 오프셋보다 작으면 커밋된 오프셋부터 실제 처리한 오프셋까지의 메시지는 중복으로 처리가 된다.\n반대로 커밋된 오프셋이 실제 처리한 오프셋보다 크면 커밋된 오프셋부터 실제 처리한 오프셋까지의 메시지는 누락된다. \n커밋방법에는 자동커밋과 수동커밋 두가지 방법이 있다.\n1) 자동커밋\n5초마다 컨슈머는 poll()을 호출할 때 가장 마지막 오프셋을 커밋한다. 커밋되기전 리밸런스가 일어나면 중복 데이터가 발생 할 수 있다.\n2) 수동커밋\n커밋을 원하는 시점에 commitSync() 를 호출해 커밋한다. 역시나 중복 데이터가 발생할 수 있지만 손실은 없다.\n\n",
        "url": "/kafka/2020/11/29/kafka-one/"
      },
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Elements",
    "excerpt": "A demo of Markdown and HTML includes\n",
    "content": "Heading 1\n\nHeading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6\n\nA small element\n\nA link\n\nLorem ipsum dolor sit amet, consectetur adip* isicing elit, sed do eiusmod *tempor incididunt ut labore et dolore magna aliqua.\n\nDuis aute irure dolor in A link reprehenderit in voluptate velit esse cillum bold text dolore eu fugiat nulla pariatur. Excepteur span element sint occaecat cupidatat non proident, sunt italicised text in culpa qui officia deserunt mollit anim id some code est laborum.\n\n\n  An item\n  An item\n  An item\n  An item\n  An item\n\n\n\n  Item one\n  Item two\n  Item three\n  Item four\n  Item five\n\n\n\n  A simple blockquote\n\n\nSome HTML…\n\n&lt;blockquote cite=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;\n  &lt;p&gt;You planning a vacation, Mr. Sullivan?&lt;/p&gt;\n  &lt;footer&gt;\n    &lt;a href=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;Sunways Security Guard&lt;/a&gt;\n  &lt;/footer&gt;\n&lt;/blockquote&gt;\n\n\n…CSS…\n\nblockquote {\n  text-align: center;\n  font-weight: bold;\n}\nblockquote footer {\n  font-size: .8rem;\n}\n\n\n…and JavaScript\n\nconst blockquote = document.querySelector(\"blockquote\")\nconst bolden = (keyString, string) =&gt;\n  string.replace(new RegExp(keyString, 'g'), '&lt;strong&gt;'+keyString+'&lt;/strong&gt;')\n\nblockquote.innerHTML = bolden(\"Mr. Sullivan\", blockquote.innerHTML)\n\n\nSingle line of code\n\nHTML Includes\n\nContact form\n\n\n  \n    Contact\n    Name: *\n    \n\n    Email Address: *\n    \n\n    Message: *\n    \n\n    \n    \n    * indicates a required field\n\n    \n      \n      \n      \n    \n  \n\n\n\n\nPlease enable JavaScript to use the form.\n\n{% include site-form.html %}\n\n\nDemo map embed\n\n\n\n{% include map.html id=\"XXXXXX\" title=\"Coffee shop map\" %}\n\n\nButton include\n\nA button\n\nA button with icon  twitter\n\n\n{% include button.html text=\"A button\" link=\"https://david.darn.es\" %}\n{% include button.html text=\"A button with icon\" link=\"https://twitter.com/daviddarnes\" icon=\"twitter\" %}\n\n\nIcon include\n\ntwitter\ntwitter\n\n\n{% include icon.html id=\"twitter\" title=\"twitter\" %}\n[{% include icon.html id=\"linkedin\" title=\"twitter\" %}](https://www.linkedin.com/in/daviddarnes)\n\n\nVideo include\n\n\n  \n\n\n{% include video.html id=\"zrkcGL5H3MU\" title=\"Siteleaf tutorial video\" %}\n\n\nImage includes\n\n\n  \n  Image with caption\n\n\n\n  \n  Right aligned image\n\n\n\n  \n  Left aligned image\n\n\n\n  \n  \n\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Image with caption\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Right aligned image\" position=\"right\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Left aligned image\" position=\"left\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/1600/800?image=894\" alt=\"Image with just alt text\" %}\n\n",
    "url": "/elements/"
  },
  
  {
    "title": "작업중인 블로그",
    "excerpt": "Alembic is a starting point for Jekyll projects. Rather than starting from scratch, this boilerplate is designed to get the ball rolling immediately. Install it, configure it, tweak it, push it.\n",
    "content": "안녕하세요.\n\nFork it  github\n\n Buy me a coffee ☕️\n Tweet it  twitter\n\n Install Alembic ⚗️\n\nFeatures\n\n\n  Available as a theme gem and GitHub Pages theme\n  Clear and elegant design that can be used out of the box or as solid starting point\n  Tested in all major browsers, including IE and Edge\n  Built in Service Worker so it can work offline and on slow connections\n  Configurable colours and typography in a single settings file\n  Extensive set of shortcodes to include various elements; such as buttons, icons, figure images and more\n  Solid typographic framework from Sassline\n  Configurable navigation via a single file\n  Modular Jekyll components\n  Post category support in the form of a single post index page grouped by category\n  Built in live search using JavaScript\n  Contact form built in using Formspree\n  Designed with Siteleaf in mind\n  Has 9 of the most popular networks as performant sharing buttons\n  Has documentation\n\n\nExamples\n\nHere are a few examples of Alembic out in the wild being used in a variety of ways:\n\n\n  bawejakunal.github.io\n  case2111.github.io\n  karateca.org\n\n\nInstallation\n\nQuick setup\n\nTo give you a running start I’ve put together some starter kits that you can download, fork or even deploy immediately:\n\n\n  ⚗️🍨 Vanilla Jekyll starter kit\n\n  ⚗️🌲 Forestry starter kit\n\n\n  \n    ⚗️💠 Netlify CMS starter kit\n\n  \n  \n    ⚗️:octocat: GitHub Pages with remote theme kit\nDownload kit\n  \n  ⚗️🚀 Stackbit starter kit\n\n\n\nAs a Jekyll theme\n\n\n  Add gem \"alembic-jekyll-theme\" to your Gemfile to add the theme as a dependancy\n  Run the command bundle install in the root of project to install the theme and its dependancies\n  Add theme: alembic-jekyll-theme to your _config.yml file to set the site theme\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nAs a GitHub Pages remote theme\n\n\n  Add gem \"jekyll-remote-theme\" to your Gemfile to add the theme as a dependancy\n  Run the command bundle install in the root of project to install the jekyll remote theme gem as a dependancy\n  Add jekyll-remote-theme to the list of plugins in your _config.yml file\n  Add remote_theme: daviddarnes/alembic to your _config.yml file to set the site theme\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nAs a Boilerplate / Fork\n\n(deprecated, not recommended)\n\n\n  Fork the repo\n  Replace the Gemfile with one stating all the gems used in your project\n  Delete the following unnecessary files/folders: .github, LICENSE, screenshot.png, CNAME and alembic-jekyll-theme.gemspec\n  Run the command bundle install in the root of project to install the jekyll remote theme gem as a dependancy\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nCustomising\n\nWhen using Alembic as a theme means you can take advantage of the file overriding method. This allows you to overwrite any file in this theme with your own custom file, by matching the file name and path. The most common example of this would be if you want to add your own styles or change the core style settings.\n\nTo add your own styles copy the styles.scss into your own project with the same file path (assets/styles.scss). From there you can add your own styles, you can even optionally ignore the theme styles by removing the @import \"alembic\"; line.\n\nIf you’re looking to set your own colours and fonts you can overwrite them by matching the variable names from the _settings.scss file in your own styles.scss, make sure to state them before the @import \"alembic\"; line so they take effect. The settings are a mixture of custom variables and settings from Sassline - follow the link to find out how to configure the typographic settings.\n",
    "url": "/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

